{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Old Tweets of the #eredivisie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you run this document with 'Cell -> Run All', the document will scrape the following tweets to a CSV:\n",
    "<br>\n",
    "<b>Explanation:</b> <p>\n",
    "We will scrape tweets from twitter.com with hashtags of the 18 eredivisie clubs to see if the conversation on twitter changed about football clubs in the Netherlands due to COVID-19<p>\n",
    "<br>\n",
    "<b>Dates:</b> <p>\n",
    "<i>Season 19/20</i><p>\n",
    "Period 1: Round 14 - 22 23 24 november 2019 <p>\n",
    "Period 2: Round 20 - 24 25 26 januari 2020<p>\n",
    "<br>\n",
    "<i>Season 20/21</i><p>\n",
    "Period 1: Round 10 - 27 28 29 november 2020<p>\n",
    "Period 2: Round 18 - 22 23 24 januari 2021 <p>\n",
    "<br>\n",
    "<b>Hashtags:</b><p>\n",
    "#AdoDenHaag\n",
    "#AFCAjax\n",
    "#AZalkmaar\n",
    "#FCEmmen\n",
    "#FCGroningen\n",
    "#FCTwente\n",
    "#FCUtrecht\n",
    "#Feyenoord\n",
    "#FortunaSittard\n",
    "#Heracles\n",
    "#PECZwolle\n",
    "#PSV\n",
    "#RKCWaalwijk\n",
    "#SCHeerenveen\n",
    "#SpartaRotterdam\n",
    "#Vitesse\n",
    "#VVVVenlo\n",
    "#WillemII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now running some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\stan\\appdata\\local\\temp\\pip-req-build-3w56yiug\n",
      "Requirement already satisfied (use --upgrade to upgrade): snscrape==0.3.5.dev96+g47fbc2a from git+https://github.com/JustAnotherArchivist/snscrape.git in c:\\users\\stan\\ananconda3.8\\lib\\site-packages\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2.24.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.6.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.9.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2020.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from beautifulsoup4->snscrape==0.3.5.dev96+g47fbc2a) (2.0.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py): started\n",
      "  Building wheel for snscrape (setup.py): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.3.5.dev96+g47fbc2a-py3-none-any.whl size=50496 sha256=5c4fcc74a21af463d2304ee0d47738d8e4c70891741e8c0e3a7cf963774fb4b7\n",
      "  Stored in directory: C:\\Users\\stan\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-gu7r2tix\\wheels\\92\\42\\87\\33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
      "Successfully built snscrape\n",
      "Requirement already satisfied: snscrape in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (0.3.5.dev96+g47fbc2a)\n",
      "Requirement already satisfied: lxml in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape) (4.6.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape) (2.24.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape) (4.9.3)\n",
      "Requirement already satisfied: pytz; python_version < \"3.9.0\" in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from snscrape) (2020.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape) (1.25.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from beautifulsoup4->snscrape) (2.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: dropbox in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (11.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from dropbox) (1.15.0)\n",
      "Requirement already satisfied: stone>=2.* in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from dropbox) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.16.2 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from dropbox) (2.24.0)\n",
      "Requirement already satisfied: ply>=3.4 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from stone>=2.*->dropbox) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests>=2.16.2->dropbox) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests>=2.16.2->dropbox) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests>=2.16.2->dropbox) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (from requests>=2.16.2->dropbox) (1.25.11)\n",
      "Requirement already satisfied: pathlib in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\stan\\ananconda3.8\\lib\\site-packages (2020.10.15)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/MartinBeckUT/TwitterScraper/blob/master/snscrape/python-wrapper/snscrape-python-wrapper.ipynb\n",
    "\n",
    "# Run the pip install command below if you don't already have the library\n",
    "\n",
    "# Installs\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "!pip install snscrape\n",
    "!pip install pandas\n",
    "!pip install dropbox\n",
    "!pip install pathlib\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import pathlib\n",
    "import dropbox\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping\n",
    "\n",
    "def get_twitterdata(filename, \n",
    "                    season,\n",
    "                    period,\n",
    "                    maxTweets = 100,\n",
    "                    searchq = ['#afcajax'],\n",
    "                    since = '2021-01-01', until='2021-01-03'):\n",
    "    \n",
    "    # Begin with X tweets to search in order to see if it works without taking too much time\n",
    "    timefilter= 'since:' + since +' until:'+until\n",
    "    print(\"Hi! I'm starting up!\")\n",
    "    print('for time period: '+ timefilter)\n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list1 = [] \n",
    "\n",
    "    # Using SNScraper to scrape data and append tweets to list\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq + ' ' + timefilter).get_items()):\n",
    "        if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "            break\n",
    "            \n",
    "        tweets_list1.append([season, period, tweet.url, tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.user.location, tweet.user.followersCount, tweet.user.friendsCount, tweet.source]) #add all the items we want to subtrect\n",
    "    print('Done scraping! Saving now :)')\n",
    "    \n",
    "    # Creating a dataframe from the tweets list above\n",
    "    df_tweets1 = pd.DataFrame(tweets_list1, columns=['Season', 'Period','URL', 'Datetime', 'Tweet Id', 'Text', 'Username', 'Replies', 'Retweets', 'Likes', 'Location', 'Followers', 'Friends', 'Source']) #give the collumns names and create our Data Frame\n",
    "    df_tweets1.to_csv(filename, sep=',', index=False)\n",
    "    \n",
    "    print(f'Done with data from {season} {period}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now scraping tweets for Eredivisie Hashtags, Season X and Period X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm starting up!\n",
      "for time period: since:2019-11-22 until:2019-11-24\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season19/20 period1!\n",
      "Hi! I'm starting up!\n",
      "for time period: since:2020-01-24 until:2020-01-26\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season19/20 period2!\n",
      "Hi! I'm starting up!\n",
      "for time period: since:2020-11-27 until:2020-11-29\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season20/21 period1!\n",
      "Hi! I'm starting up!\n",
      "for time period: since:2021-01-22 until:2021-01-24\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season20/21 period2!\n",
      "all done! :)\n"
     ]
    }
   ],
   "source": [
    "# Season 19/20, period 1\n",
    "get_twitterdata(filename= 'season19-20period1max100.csv',\n",
    "                searchq= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII',\n",
    "                since = '2019-11-22',\n",
    "                until = '2019-11-24',\n",
    "                season= 'season19/20',\n",
    "                period= 'period1')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 19/20, period 2\n",
    "get_twitterdata(filename= 'season19-20period2max100.csv',\n",
    "                searchq= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII',\n",
    "                since = '2020-01-24',\n",
    "                until = '2020-01-26',\n",
    "                season= 'season19/20',\n",
    "                period= 'period2')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 1\n",
    "get_twitterdata(filename= 'season20-21period1max100.csv',\n",
    "                searchq= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII',\n",
    "                since = '2020-11-27', \n",
    "                until = '2020-11-29',\n",
    "                season= 'season20/21',\n",
    "                period= 'period1')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 2\n",
    "get_twitterdata(filename= 'season20-21period2max100.csv',\n",
    "                searchq= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII',\n",
    "                since = '2021-01-22',\n",
    "                until = '2021-01-24',\n",
    "                season= 'season20/21',\n",
    "                period= 'period2')\n",
    "\n",
    "print(\"all done! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now merging and exporting local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done! twitter data available in your directory\n"
     ]
    }
   ],
   "source": [
    "Find_All_CSVs = ['season19-20period1max100.csv',\n",
    "                 'season19-20period2max100.csv',\n",
    "                 'season20-21period1max100.csv',\n",
    "                 'season20-21period2max100.csv']\n",
    "\n",
    "#combine all files in the list\n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f,header=0) for f in Find_All_CSVs])\n",
    "\n",
    "combined_csv.head()\n",
    "\n",
    "combined_csv.to_csv(\"merged_tweets_eredivisiemax100.csv\", quotechar='\"',\n",
    "          quoting=csv.QUOTE_ALL, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"all done! twitter data available in your directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now exporting to Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done, check out the dropbox folder: \n"
     ]
    }
   ],
   "source": [
    "class TransferData:\n",
    "    def __init__(self, access_token):\n",
    "        self.access_token = access_token\n",
    "\n",
    "    def upload_file(self, file_from, file_to):\n",
    "        \"\"\"upload a file to Dropbox using API v2\n",
    "        \"\"\"\n",
    "        dbx = dropbox.Dropbox(self.access_token)\n",
    "\n",
    "        with open(file_from, 'rb') as f:\n",
    "            dbx.files_upload(f.read(), file_to)\n",
    "\n",
    "def main():\n",
    "    access_token = 'Dropbox-Token'\n",
    "    transferData = TransferData(access_token)\n",
    "\n",
    "    file_from = 'merged_tweets_eredivisie.csv'\n",
    "    file_to = '/Apps/dPrep/merged_tweets_eredivisie.csv'  # The full path to upload the file to, including the file name\n",
    "\n",
    "    # API v2\n",
    "    transferData.upload_file(file_from, file_to)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "print(\"All done, check out the dropbox folder: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
